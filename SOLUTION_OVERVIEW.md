# Giáº£i phÃ¡p crawl dá»¯ liá»‡u FC Online - Tá»•ng quan

## ğŸ¯ BÃ i toÃ¡n

Crawl dá»¯ liá»‡u cáº§u thá»§ tá»« https://automua.com/players vÃ  lÆ°u vÃ o MongoDB theo yÃªu cáº§u:

1. âœ… LÆ°u thÃ´ng tin tá»«ng cáº§u thá»§ (hÃ¬nh áº£nh chá»‰ lÆ°u URL)
2. âœ… Crawl theo tá»«ng mÃ¹a vÃ  vá»‹ trÃ­ Ä‘á»ƒ dá»… query
3. âœ… LÆ°u hÃ¬nh áº£nh avatar vÃ  cÃ¡c chá»‰ sá»‘ áº©n
4. âœ… LÆ°u táº¥t cáº£ thÃ´ng tin ká»ƒ cáº£ sá»± nghiá»‡p CLB
5. âœ… Viáº¿t báº±ng Node.js, lÆ°u vÃ o MongoDB

## ğŸ”§ Giáº£i phÃ¡p

### Kiáº¿n trÃºc tá»•ng thá»ƒ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            User Interface (CLI)              â”‚
â”‚  npm start season EL                         â”‚
â”‚  npm start position ST EL                    â”‚
â”‚  npm start custom ST,LW EL,ICON              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Crawler Service                      â”‚
â”‚  - Orchestrate crawling process              â”‚
â”‚  - Handle position x season combinations     â”‚
â”‚  - Track statistics                          â”‚
â”‚  - Error isolation per player                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â†“                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Player List     â”‚  â”‚  Player Detail   â”‚
â”‚  Scraper         â”‚  â”‚  Scraper         â”‚
â”‚                  â”‚  â”‚                  â”‚
â”‚  Input:          â”‚  â”‚  Input:          â”‚
â”‚  - Position      â”‚  â”‚  - Player URL    â”‚
â”‚  - Seasons       â”‚  â”‚                  â”‚
â”‚                  â”‚  â”‚  Output:         â”‚
â”‚  Output:         â”‚  â”‚  - Stats         â”‚
â”‚  - Player list   â”‚  â”‚  - Hidden stats  â”‚
â”‚  - Basic info    â”‚  â”‚  - Club career   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                     â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚    HTTP Client       â”‚
         â”‚  - Retry logic       â”‚
         â”‚  - Rate limiting     â”‚
         â”‚  - Exponential       â”‚
         â”‚    backoff           â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Player Repository   â”‚
         â”‚  - Upsert logic      â”‚
         â”‚  - Check existence   â”‚
         â”‚  - Index management  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚      MongoDB         â”‚
         â”‚                      â”‚
         â”‚  Collection:         â”‚
         â”‚    players           â”‚
         â”‚                      â”‚
         â”‚  Indexes:            â”‚
         â”‚    playerId+season   â”‚
         â”‚    name, position    â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Workflow chi tiáº¿t

#### 1. Crawl Player List

```
URL: https://automua.com/players?positions[0]=ST&seasons[0]=EL
                    â†“
         [HTTP GET with retry]
                    â†“
         [Parse HTML with Cheerio]
                    â†“
         Extract for each player:
         - playerId (from URL)
         - name (from .text-truncate)
         - avatarUrl (from img src)
         - season (from .season-badge)
         - positions & ratings
         - overallRating (from .hexagon-text)
         - starRating (count .fa-star)
                    â†“
         Return array of players
```

#### 2. Crawl Player Detail

```
URL: https://automua.com/players/cristiano-ronaldo-zzwyoyoy
                    â†“
         [HTTP GET with retry]
                    â†“
         [Parse HTML with Cheerio]
                    â†“
         Extract Stats:
         #playerTabsContent â†’ .card-body â†’ .d-flex
         For each stat row:
         - Stat name (from .small)
         - Value, baseValue, originalValue
                    â†“
         Extract Hidden Stats:
         .row.row-cols-2.row-cols-md-3 â†’ .col
         For each trait:
         - Name, description, iconUrl
                    â†“
         Extract Club Career:
         h5:contains("Sá»± nghiá»‡p CLB") â†’ .border-bottom
         Parse period and club name
                    â†“
         Return complete player data
```

#### 3. Save to Database

```
Player data from scraper
         â†“
Check if exists (playerId + season)
         â†“
   â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
   â”‚           â”‚
   â†“           â†“
Exists?    Not exists?
   â”‚           â”‚
   â†“           â†“
UPDATE      INSERT
   â”‚           â”‚
   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
         â†“
Set updatedAt timestamp
         â†“
Return upsert result
```

## ğŸ“¦ Modules vÃ  chá»©c nÄƒng

### Core Modules

| Module | File | Chá»©c nÄƒng |
|--------|------|-----------|
| **Config** | constants.js | Positions, seasons, configs |
| **Connection** | connection.js | MongoDB connection manager |
| **Repository** | playerRepository.js | CRUD operations |
| **HTTP Client** | httpClient.js | HTTP with retry & rate limit |
| **List Scraper** | playerListScraper.js | Scrape player list pages |
| **Detail Scraper** | playerDetailScraper.js | Scrape player detail pages |
| **Crawler Service** | crawlerService.js | Orchestration logic |
| **Logger** | logger.js | Logging utility |
| **Delay** | delay.js | Rate limiting utility |
| **Entry Point** | index.js | CLI interface |

### Utility Functions

#### httpClient.fetchWithRetry()
- Retry up to MAX_RETRIES times
- Exponential backoff: delay Ã— (attempt)
- Custom headers to mimic browser
- Rate limiting with configurable delay

#### playerListScraper.extractPlayerList()
- Parse HTML structure: `.d-flex.align-items-center`
- Extract player card info
- Build proper player URLs
- Handle missing data gracefully

#### playerDetailScraper.extractGeneralStats()
- Find `#playerTabsContent #all-stats`
- Extract stat name + values (value, base, original)
- Return structured stats object

#### playerDetailScraper.extractHiddenStats()
- Find `.row.row-cols-2.row-cols-md-3`
- Extract trait name, description, icon
- Return array of traits

#### playerDetailScraper.extractClubCareer()
- Find "Sá»± nghiá»‡p CLB" section
- Parse period and club name
- Handle various formats (single year, range)

#### crawlerService.crawlPositionAndSeason()
1. Get player list
2. For each player:
   - Check if exists (skip if skipExisting=true)
   - Scrape detail
   - Merge data
   - Upsert to DB
3. Track and return statistics

## ğŸ—ƒï¸ Database Design

### Collection: players

**Indexes:**
```javascript
{ playerId: 1, season: 1 }  // Unique composite
{ name: 1 }                  // Search by name
{ position: 1 }              // Filter by position
{ season: 1 }                // Filter by season
{ createdAt: 1 }             // Sort by time
```

**Document Structure:**
```javascript
{
  // Primary keys
  playerId: String,      // Unique ID from URL
  season: String,        // Season code
  
  // Basic info
  name: String,
  position: String,
  playerUrl: String,
  avatarUrl: String,
  mainImageUrl: String,
  
  // Ratings
  overallRating: Number,
  starRating: Number,
  
  // Positions this player can play
  positions: [{
    position: String,
    rating: String
  }],
  
  // All stats (30+ attributes)
  stats: {
    "Tá»‘c Ä‘á»™": {
      value: Number,
      baseValue: Number,
      originalValue: Number
    },
    // ... more stats
  },
  
  // Hidden stats/traits
  hiddenStats: [{
    name: String,
    description: String,
    iconUrl: String
  }],
  
  // Club career history
  clubCareer: [{
    period: String,     // "2023" or "2021 - 2022"
    club: String        // Club name
  }],
  
  // Metadata
  scrapedAt: Date,
  createdAt: Date,
  updatedAt: Date
}
```

## ğŸ”„ Error Handling Strategy

### Layer 1: HTTP Level
```javascript
try {
  const response = await axios.get(url)
  return response.data
} catch (error) {
  if (retries > 0) {
    await delay(backoff)
    return fetchWithRetry(url, retries - 1)
  }
  throw error
}
```

### Layer 2: Scraper Level
```javascript
try {
  const html = await fetchWithRetry(url)
  return extractData(html)
} catch (error) {
  logger.error('Scraping failed', error)
  return [] // or null
}
```

### Layer 3: Service Level
```javascript
for (const player of players) {
  try {
    const data = await scrapeDetail(player.url)
    await upsertPlayer(data)
    stats.success++
  } catch (error) {
    logger.error('Player failed', error)
    stats.failed++
    // Continue with next player
  }
}
```

### Layer 4: Application Level
```javascript
process.on('unhandledRejection', (error) => {
  logger.error('Unhandled error', error)
  process.exit(1)
})
```

## âš™ï¸ Configuration

### Environment Variables (.env)

```bash
# MongoDB
MONGODB_URI=mongodb://localhost:27017/fconline
DB_NAME=fconline
PLAYERS_COLLECTION=players

# Crawler behavior
REQUEST_DELAY=1000      # Delay between requests (ms)
MAX_RETRIES=3           # Max retry attempts

# Target site
BASE_URL=https://automua.com
```

### Hardcoded Constants

```javascript
// Positions: 15 vá»‹ trÃ­
['ST', 'LW', 'RW', 'CF', 'CAM', 'LM', 'RM', 'CM', 
 'CDM', 'LWB', 'RWB', 'LB', 'RB', 'CB', 'GK']

// Seasons: 100+ mÃ¹a giáº£i
['ICONTM', 'ICON', 'EL', '25TY', '24TY', ...]

// Request config
{
  delay: 1000,
  maxRetries: 3,
  timeout: 30000
}
```

## ğŸ“Š Performance Characteristics

### Throughput
- ~1 request per second (with default delay)
- ~60 players per minute
- ~3600 players per hour (theoretical max)

### Resource Usage
- Memory: ~100-200 MB
- CPU: Low (IO-bound)
- Network: ~1-5 requests/second
- Disk: ~5-10 KB per player

### Scalability
- Sequential by default (reliable)
- Can be parallelized (needs tuning)
- Bottleneck: Network + rate limiting
- Database: Can handle millions of documents

## ğŸ¯ Key Design Decisions

### 1. URL Format
**Quyáº¿t Ä‘á»‹nh**: Sá»­ dá»¥ng `positions[0]` vÃ  `seasons[0]` format

**LÃ½ do**: Website yÃªu cáº§u array index format, khÃ´ng cháº¥p nháº­n `positions=ST`

### 2. Composite Primary Key
**Quyáº¿t Ä‘á»‹nh**: `{ playerId, season }` lÃ  unique key

**LÃ½ do**: CÃ¹ng 1 cáº§u thá»§ cÃ³ thá»ƒ cÃ³ nhiá»u card khÃ¡c nhau theo mÃ¹a giáº£i

### 3. URL-only for Images
**Quyáº¿t Ä‘á»‹nh**: Chá»‰ lÆ°u URL, khÃ´ng download images

**LÃ½ do**: 
- Tiáº¿t kiá»‡m disk space
- Táº­n dá»¥ng CDN cá»§a automua.com
- Faster crawling

### 4. Sequential Processing
**Quyáº¿t Ä‘á»‹nh**: Crawl tuáº§n tá»±, khÃ´ng parallel

**LÃ½ do**:
- Respect rate limits
- Easier error handling
- More reliable
- Simpler code

### 5. Skip Existing by Default
**Quyáº¿t Ä‘á»‹nh**: Máº·c Ä‘á»‹nh skip players Ä‘Ã£ cÃ³ trong DB

**LÃ½ do**:
- Avoid redundant work
- Faster incremental updates
- Option to force update when needed

### 6. Upsert Pattern
**Quyáº¿t Ä‘á»‹nh**: Sá»­ dá»¥ng upsert thay vÃ¬ insert

**LÃ½ do**:
- Idempotent operations
- Safe to re-run
- Automatic updates

## ğŸ› ï¸ Usage Patterns

### Pattern 1: Initial Full Crawl
```bash
# Crawl all positions for important seasons
npm start season EL
npm start season ICON
npm start season 25TY
```

### Pattern 2: Position-specific Update
```bash
# Update strikers only
npm start position ST EL
npm start position ST ICON
```

### Pattern 3: Custom Combinations
```bash
# Attack positions for multiple seasons
npm start custom ST,LW,RW,CF EL,ICON,25TY
```

### Pattern 4: Force Re-crawl
```bash
# Update existing data
npm start season EL --force
```

## ğŸ“ˆ Statistics Tracking

For each crawl operation, tracks:
```javascript
{
  position: "ST",
  season: "EL",
  playersFound: 50,      // Found in list
  playersScraped: 48,    // Successfully scraped
  playersSkipped: 2,     // Already in DB
  playersFailed: 0       // Failed to scrape
}
```

## âœ… Testing Strategy

### Unit-level
- Test individual extractors with sample HTML
- Mock HTTP requests
- Test error handling

### Integration-level
- `examples/testScraper.js` - Test scrapers
- `examples/quickStart.js` - Test end-to-end

### Production-level
- Start with small crawls
- Monitor logs
- Verify data in MongoDB
- Gradually increase scope

## ğŸ” Best Practices Implemented

1. âœ… **Separation of Concerns** - Each module has single responsibility
2. âœ… **Error Isolation** - One failure doesn't break entire crawl
3. âœ… **Retry Logic** - Automatic retry with backoff
4. âœ… **Rate Limiting** - Configurable delay between requests
5. âœ… **Logging** - Comprehensive logging for debugging
6. âœ… **Idempotency** - Safe to re-run same crawl
7. âœ… **Graceful Shutdown** - Handle SIGINT/SIGTERM properly
8. âœ… **Database Indexes** - Optimize query performance
9. âœ… **Configuration** - Environment-based config
10. âœ… **Documentation** - Extensive docs for maintainability

## ğŸš€ Production Ready Checklist

- âœ… Error handling at all levels
- âœ… Logging for debugging
- âœ… Configuration via environment
- âœ… Database indexes
- âœ… Retry logic
- âœ… Rate limiting
- âœ… Graceful shutdown
- âœ… Documentation
- âœ… Examples
- âœ… Testing utilities

**Status: READY FOR PRODUCTION USE** âœ…

---

**Káº¿t luáº­n**: Giáº£i phÃ¡p hoÃ n chá»‰nh, production-ready, cÃ³ thá»ƒ crawl hÃ ng nghÃ¬n cáº§u thá»§ má»™t cÃ¡ch tin cáº­y vÃ  hiá»‡u quáº£! ğŸ‰
